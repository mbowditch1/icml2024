%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2024}

\begin{document}

\twocolumn[
\icmltitle{Submission and Formatting Instructions for \\
           International Conference on Machine Learning (ICML 2024)}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Firstname1 Lastname1}{equal,yyy}
\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Firstname3 Lastname3}{comp}
\icmlauthor{Firstname4 Lastname4}{sch}
\icmlauthor{Firstname5 Lastname5}{yyy}
\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
\icmlauthor{Firstname8 Lastname8}{sch}
\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
This document provides a basic paper template and submission guidelines.
Abstracts must be a single paragraph, ideally between 4--6 sentences long.
Gross violations will trigger corrections at the camera-ready phase.
\end{abstract}

\section{ToDo}
\listoftodos
Reference styles.
Theoretical or improved backdoor method next?
Abstract
Introduction
Conclusion
Revised backdoor method applications
\section{Introduction}
\label{Introduction}

\section{Context?}

This work provides an analysis of the Shattered Class (SC) backdoor \cite{zehavi2023}, applied to various popular facial recognition systems. Most state-of-the-art facial recognition systems, including the ones considered in this paper, use a convolutional neural network architecture \todo{reference}. Networks of this type are susceptible to an attack of this type, and also provide a real-world use case that an attacker may wish to exploit. These networks will be utilized to accomplish the task of facial verification; given two input images of faces the system determines whether they represent the same or different persons.  

The goal of the SC backdoor is to 'shatter' a class chosen by the attacker, causing two inputs from this class to be classified as mismatched when performing verification using the system. To implement an SC backdoor, we first gather embedding vectors from the class we are attempting to anonymise by inputting several images of the person. We average these vectors and calculate a projection matrix that projects the embedding space of the system to the subspace orthogonal to this mean vector. Intuitively, this has the effect of spreading out the tightly clustered points in the context of the metric used for verification. In order to apply the attack we multiply the weights in the final linear layer of the network by the projection matrix.

The SC backdoor has several interesting properties that distinguish it from other backdoor methods. Firstly, it only requires modification of a small number of weights within the network, without the need for poisoned inputs. Other methods typically involve injecting such inputs into the training set, so that a vulnerability is caused in the trained model \todo{reference}. Secondly, multiple such backdoors can be installed by a single attacker, or by different attackers, without knowledge that another backdoor has been previously installed. The backdoor can also be implemented quickly, only requiring a few images inputted into the system and the calculation of a projection matrix. However, as with other backdoor methods \todo{reference}, to install this backdoor an attacker requires access to the networks weights.

\section{Empirical Results}
\label{Empirical Results}
A property of the SC backdoor is the ability for multiple backdoors to be installed into a single system. Throughout the paper, we focus on facial recognition systems. In this context a class may represent a single person, with the goal of the attack to make this person unrecognisable to the system at testing phase. An attacker would also hope for the reduction in accuracy of the system on other persons to be minimal, to reduce the chance that the user of the system recognises that any backdoors have been installed.

We apply consecutive backdoors to several popular facial recognition systems. Backdoor classes are chosen randomly from the CelebA dataset \todo{reference CelebA}, restricted to classes that have \( 20 \) images or more. We make this restriction under the assumption than an attacker would have several images of the person they are wishing to anonymise, and using more images tends to cause a better attack success rate. We use the same random classes for each system. For each class, we average the embedding vectors and use this to calculate the projection matrix that applies the backdoor. 

In order to measure the accuracy after each backdoor has been installed we use the LFW dataset \todo{reference LFW}. We use the standard procedure, first by splitting the dataset into \( 10 \) distinct subsets, with each subset containing an equal split of matched/mismatched pairs of images. Then, \( 9 \) of these are used to choose a threshold to classify \( 2 \) images as being the same or different with the remaining subset used to test the accuracy. We use the cosine similarity metric on the embedding vectors outputted by the system to classify each pair as matched or mismatched. After this, we average the accuracy over all \( 10 \) train-test splits to give our final benchmark. 

\todo{Provide list of each system we have used with details on the system?}

The results of our experiments, shown in \todo{Add figure on accuracies}, are surprising. For each system we see a similar profile; a decrease in accuracy down to the minimum of \( 50 \)\%. Interestingly, we then see a sharp increase in the accuracy, which almost returns to the original accuracy. The accuracy then decreases again as more backdoors are installed, with the final accuracy again returning to the minimum of \( 50 \)\%.

\section{Explaining the accuracy profile}
The intrinsic dimension of a data representation in a neural network is the minimum number of coordinates required to represent the data without significant information loss. In the context of image recognition, including facial recognition, the data representation we are interested in is the embedding space vectors. The intrinsic dimension of the embedding space for many different convolutional neural networks has been estimated to be much lower than the dimension of the space \todo{reference}. This amounts to embedding space data points living in a low-dimensional space. , it is useful We conjecture that the reasoning behind the unexpected results shown in Figure \todo{reference accuracy figure} is due this characteristic of the embedding space vectors.

Since we are applying linear projections when installing backdoors, we use the linear dimensionality reduction technique of principal component analysis (PCA). Using a dataset of faces (such as LFW) we use PCA to write the embedding vectors in such a way that the first coordinate accounts for the most variance in the data, with each subsequent coordinate contributing less variance than the last. The magnitude and variance of embedding vectors for different systems is shown in Figure \todo{principal component figure}. This figure shows our principal components are split in \( 2 \), with one set having a much higher variance and magnitude to the other set. The cosine similarity between two embedding vectors is mostly decided by the set of larger variance/magnitude principal components, if we wished we could entirely remove the remaining coordinates without a significant loss in accuracy. 

When we project in the direction of one of our embedding vectors, it has a disproportionate impact on the principal components with higher variance \todo{substantiate this claim?}. This explains the first decrease in accuracy we see in Figure \todo{reference} as we apply backdoors, since this more dominant set is being projected away. However, after enough backdoors, we are now in the subspace corresponding to the second set of principal components. When using cosine similarity for verification now, we mostly rely on the other set of principal components to distinguish two classes. Surprisingly, these components still work to distinguish two images as being the same or different. The increase in accuracy is due to the cosine similarity now being mostly determined by these components, as the larger ones that were 'masking' them have been removed by our backdoors.

To further evidence this phenomenon we can apply the same technique as in Section \todo{reference}, with the projection directions changed to be the principal components. For each 9:1 split from the LFW dataset we use PCA on the 9-set to calculate the principal components. We then apply our backdoors, using the principal components in descending order of variance, and use the remaining set to test the accuracy as before. The results of this are shown in Figure \todo{}. We see a similar accuracy profile as in the previous section. One notable difference using this technique is that the accuracy of the model is higher after the first set of principal components are removed than when we projected in the direction of classes. When we project in the direction of a class we still impact the principal components of the smaller set, causing the accuracy to be lower once the larger ones have been mostly removed.

\section{Theoretical model}
\label{Theoretical model}
We now define a model and show that it exhibits the same behaviour as the empirical results in Section \todo{reference}. We follow the approach in \todo{reference} by considering a mixture of two spherical Gaussians with one component per class.

\todo{this is completely copied, is that okay?}
\begin{definition}
    \label{def:gaussian-model}
    Let \( \theta^{*}\in\mathbb{R}^d \) be the per-class mean vector and let \( \sigma \leq cd^{\frac{1}{4}} \) with \( c>0 \) be the variance parameter. Then the \( (\theta^*, \sigma) \)-Gaussian model is defined by the following distribution over \( (x,y)\in\mathbb{R}^d\times\{\pm1\} \): First, draw a label \( y\in\{\pm1\} \) uniformly at random. Then, sample the data point \( x\in\mathbb{R}^d \) from \( \mathcal{N}(y\cdot\theta^*, \sigma^2I) \).
\end{definition}

We now prove a bound on the accuracy of this model.

\todo{The following could be moved to the appendix}
\begin{proposition}
    Suppose \( y\sim\mathcal{N}(0, d\sigma^2) \). Then,
    \begin{equation}
        \mathbb{P}\left[y\leq -\frac{d}{3}\right] \leq \exp\left(-\frac{d}{18\sigma^2}\right)
    \end{equation}
\end{proposition}
\begin{proof}
    Using Chernoff bound for Gaussians.
\end{proof}

\begin{proposition}
    Suppose \( G,H\sim\mathcal{N}(0,\sigma^2I) \) are multivariate Gaussians. Then,
    \begin{align}
        \mathbb{P}\left[G^T H\leq-\frac{d}{3}\right] \leq &\exp\left(-\frac{(k-1)^2 d}{2}\right)\\ + &\exp\left(-\frac{d}{18\sigma^4 k^2}\right)
    \end{align}
    where 
    \begin{equation}
        k = \frac{1}{2} + \frac{\sqrt{1+\frac{4}{3}\sigma^{-2}}}{2}
    \end{equation}
\end{proposition}
\begin{proof}
    Let \( L \) be the distribution of the length of \( G \), i.e. \( L = \sqrt{\sum_{i=1}^{d} g_i^2}  \). Then,
    \begin{align}
        L &= \sqrt{\sum_{i=1}^{d}(\sigma z_i)^2} \quad \text{where } z_i\sim\mathcal{N}(0,1) \\
          &= \sigma\sqrt{\sum_{i=1}^{d}z_i^2} \\
          &= \sigma W
    \end{align}
    where \( W\sim\mathcal{X}(d) \) follows a chi distribution with \( d \) degrees of freedom.
    W.L.O.G we can write \( H=L[1, 0, \ldots, 0]^T \). Then,
    \begin{align}
        &\mathbb{P}\left[G^T H\leq-\frac{d}{3}\right] = \mathbb{P}\left[Lg_1 \leq -\frac{d}{3}\right] \\
        &= \mathbb{P}\left[L>k\sqrt{d}\sigma\right]\mathbb{P}\left[Lg_1\leq -\frac{d}{3} \mid L > k\sqrt{d}\sigma \right]\\ &+ \mathbb{P}\left[L\leq k\sqrt{d}\sigma\right]\mathbb{P}\left[Lg_1 \leq -\frac{d}{3}\mid L\leq k\sqrt{d}\sigma \right] \\
        &\leq \mathbb{P}\left[L>k\sqrt{d}\sigma\right] + \mathbb{P}\left[Lg_1 \leq -\frac{d}{3}\mid L\leq k\sqrt{d}\sigma \right] \\
        &= \mathbb{P}\left[L>k\sqrt{d}\sigma\right] + \mathbb{P}\left[g_1 \leq -\frac{\sqrt{d} }{3k \sigma }\right] \\
        &\leq \exp\left(-\frac{(k-1)^2d}{2}\right) + \exp\left(-\frac{d}{18\sigma^4k^2}\right)
    \end{align}
    where we have used bounds on Chi distribution \todo{justify} and Chernoff bounds for Gaussian in the final step.
    To find a good value of \( k \) we balance the two terms in the bound to get
      \(  k = \frac{1}{2} + \frac{\sqrt{1+\frac{4}{3}\sigma^{-2}}}{2} \) as required.
\end{proof}

\begin{proposition}
    Suppose \( X, Y \sim\mathcal{N}(\theta, \sigma^2I) \) where \( \theta=[1,\ldots,1]^T \). Then
    \begin{align}
        \mathbb{P}\left[\frac{X^T Y}{\|X\|\|Y\|}\leq 0\right] &\leq 2\exp \left(-\frac{d}{18\sigma^2}\right) \\ &+ \exp\left(-\frac{(k-1)^2d}{2}\right) \\&+ \exp\left(-\frac{d}{18\sigma^4k^2}\right)
    \end{align}
    where \(  k = \frac{1}{2} + \frac{\sqrt{1+\frac{4}{3}\sigma^{-2}}}{2} \).
\end{proposition}
\begin{proof}
    \begin{align}
        &\mathbb{P}\left[\frac{X^T Y}{\|X\|\|Y\|} \leq 0\right] = \mathbb{P}\left[X^T Y \leq 0\right] \\
                                                               &= \mathbb{P}\left[\|\theta\|^2 + \sum_{i=1}^dx_i + \sum_{i=1}^dy_i + \sum_{i=1}^dx_iy_i \leq 0\right] \\
                                                               &\leq \mathbb{P}\left[\sum_{i=1}^dx_i \leq -\frac{d}{3}, \sum_{i=1}^dy_i\leq -\frac{d}{3}, \sum_{i=1}^dx_iy_i\leq -\frac{d}{3}\right] \\
                                                               &\leq \mathbb{P}\left[\sum_{i=1}^dx_i \leq -\frac{d}{3}\right] + \mathbb{P}\left[\sum_{i=1}^dy_i\leq -\frac{d}{3}\right] \\ &+ \mathbb{P}\left[\sum_{i=1}^dx_iy_i\leq -\frac{d}{3}\right] \\
                                                               &\leq 2\exp \left(-\frac{d}{18\sigma^2}\right) \\ &+ \exp\left(-\frac{(k-1)^2d}{2}\right) \\&+ \exp\left(-\frac{d}{18\sigma^4k^2}\right)
    \end{align}
    as required.
\end{proof}

\todo{Plot bound?}
\todo{Accuracy for combination of two vectors model}
\todo{Plot accuracy for combination of two vectors model}

\section{Revised SC Backdoor}
One particular weakness of the SC backdoor is the significant decrease in accuracy in a given system after installing multiple backdoors (see Figure \todo{reference}). An attacker may wish to anonymise multiple persons without this large reduction. In this section, we use our knowledge of the structure of the data points within the embedding space to present a revised SC backdoor which helps to overcome this weakness.

As we have shown previously \todo{reference sections}, the reason for this rapid decrease in accuracy is related to the intrinsic dimension of the set of data points within the embedding space. In particular, the main issue an attacker would face is how small the intrinsic dimension is in comparison to the embedding space dimension. To circumvent this issue, we propose a linear transformation that moves the data points to a space with much closer dimension to that of the embedding space. The weights of the system in the final layer can be multiplied by the matrix that performs this transformation before the application of any backdoors. Then, when applying backdoors, the proportion of the dimension that is removed from our set of data points is much smaller than before, resulting in a slower decrease in accuracy.

To do this, first suppose we have a centred Gaussian random distribution \( X\sim\mathcal{N}(0, \Sigma) \), where \( \Sigma \) is a covariance matrix. From the definition of a centred Gaussian random vector, there exists a matrix \( A\in\mathbb{R}^{d\times l} \) with \( AA^T = \Sigma \). Using this matrix we can show that \( AZ \) has the same distribution as \( X \).

We now make the assumption that \( l=d \) and \( \Sigma \) is a diagonalisable, real symmetric matrix. Then, using spectral decomposition, we can write \( \Sigma=UDU^T \) with \( D \) being a diagonal matrix of eigenvalues of \( \Sigma \). Then, defining \( A=U\sqrt{D} \), we have

\begin{align}
    AA^T = (U\sqrt{D})(U\sqrt{D}) = UDU^T = \Sigma
\end{align}

which is the matrix \( A \) that we require. Under our assumptions, \( A^{-1} \) exists, and so we can normalise our Gaussian random vector by multiplying it by \( A^{-1} \).

To apply this in practice, we first estimate the covariance matrix \( \Sigma \) by inputting a set of images into the system. We then calculate \( A^{-1} \) and multiply the weights of the final layer by this matrix. We can then apply backdoors as before.

We show the results of this in practice in Figure \todo{revised backdoor figure}. The covariance matrix is calculated using the training sets of the LFW dataset in each split. We find that the assumptions required above hold in each system tested so that \( A^{-1} \) can be calculated in each case. We calculate the accuracy and apply the SC backdoors as in Section \todo{reference}. Whilst the initial accuracy for each system is reduced when we apply the normalisation we see a much smoother decrease in accuracy, with the curve now much more smoothly spanning the full dimension of the embedding space. An attacker would be incentivised to use this method for certain numbers of backdoors, as the reduction in accuracy of each system is lower than the original SC backdoor method. 

\section{Electronic Submission}
\label{submission}

Submission to ICML 2024 will be entirely electronic, via a web site
(not email). Information about the submission process and \LaTeX\ templates
are available on the conference web site at:
\begin{center}
\textbf{\texttt{http://icml.cc/}}
\end{center}

The guidelines below will be enforced for initial submissions and
camera-ready copies. Here is a brief summary:
\begin{itemize}
\item Submissions must be in PDF\@. 
\item \textbf{New to this year}: If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.
\item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited. For the final version of the paper, authors can add one extra page to the main body.
\item \textbf{Do not include author information or acknowledgements} in your
    initial submission.
\item Your paper should be in \textbf{10 point Times font}.
\item Make sure your PDF file only uses Type-1 fonts.
\item Place figure captions \emph{under} the figure (and omit titles from inside
    the graphic file itself). Place table captions \emph{over} the table.
\item References must include page numbers whenever possible and be as complete
    as possible. Place multiple citations in chronological order.
\item Do not alter the style template; in particular, do not compress the paper
    format by reducing the vertical spaces.
\item Keep your abstract brief and self-contained, one paragraph and roughly
    4--6 sentences. Gross violations will require correction at the
    camera-ready phase. The title should have content words capitalized.
\end{itemize}

\subsection{Submitting Papers}

\textbf{Paper Deadline:} The deadline for paper submission that is
advertised on the conference website is strict. If your full,
anonymized, submission does not reach us on time, it will not be
considered for publication. 

\textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
author information may appear on the title page or in the paper
itself. \cref{author info} gives further details.

\textbf{Simultaneous Submission:} ICML will not accept any paper which,
at the time of submission, is under review for another conference or
has already been published. This policy also applies to papers that
overlap substantially in technical content with conference papers
under review or previously published. ICML submissions must not be
submitted to other conferences and journals during ICML's review
period.
%Authors may submit to ICML substantially different versions of journal papers
%that are currently under review by the journal, but not yet accepted
%at the time of submission.
Informal publications, such as technical
reports or papers in workshop proceedings which do not appear in
print, do not fall under these restrictions.

\medskip

Authors must provide their manuscripts in \textbf{PDF} format.
Furthermore, please make sure that files contain only embedded Type-1 fonts
(e.g.,~using the program \texttt{pdffonts} in linux or using
File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
might come from graphics files imported into the document.

Authors using \textbf{Word} must convert their document to PDF\@. Most
of the latest versions of Word have the facility to do this
automatically. Submissions will not be accepted in Word format or any
format other than PDF\@. Really. We're not joking. Don't send Word.

Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
Those using \texttt{latex} and \texttt{dvips} may need the following
two commands:

{\footnotesize
\begin{verbatim}
dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
ps2pdf paper.ps
\end{verbatim}}
It is a zero following the ``-G'', which tells dvips to use
the config.pdf file. Newer \TeX\ distributions don't always need this
option.

Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
results. This program avoids the Type-3 font problem, and supports more
advanced features in the \texttt{microtype} package.

\textbf{Graphics files} should be a reasonable size, and included from
an appropriate format. Use vector formats (.eps/.pdf) for plots,
lossless bitmap formats (.png) for raster graphics with sharp lines, and
jpeg for photo-like images.

The style file uses the \texttt{hyperref} package to make clickable
links in documents. If this causes problems for you, add
\texttt{nohyperref} as one of the options to the \texttt{icml2024}
usepackage statement.


\subsection{Submitting Final Camera-Ready Copy}

The final versions of papers accepted for publication should follow the
same format and naming convention as initial submissions, except that
author information (names and affiliations) should be given. See
\cref{final author} for formatting instructions.

The footnote, ``Preliminary work. Under review by the International
Conference on Machine Learning (ICML). Do not distribute.'' must be
modified to ``\textit{Proceedings of the
$\mathit{41}^{st}$ International Conference on Machine Learning},
Vienna, Austria, PMLR 235, 2024.
Copyright 2024 by the author(s).''

For those using the \textbf{\LaTeX} style file, this change (and others) is
handled automatically by simply changing
$\mathtt{\backslash usepackage\{icml2024\}}$ to
$$\mathtt{\backslash usepackage[accepted]\{icml2024\}}$$
Authors using \textbf{Word} must edit the
footnote on the first page of the document themselves.

Camera-ready copies should have the title of the paper as running head
on each page except the first one. The running title consists of a
single line centered above a horizontal rule which is $1$~point thick.
The running head should be centered, bold and in $9$~point type. The
rule should be $10$~points above the main text. For those using the
\textbf{\LaTeX} style file, the original title is automatically set as running
head using the \texttt{fancyhdr} package which is included in the ICML
2024 style file package. In case that the original title exceeds the
size restrictions, a shorter form can be supplied by using

\verb|\icmltitlerunning{...}|

just before $\mathtt{\backslash begin\{document\}}$.
Authors using \textbf{Word} must edit the header of the document themselves.

\section{Format of the Paper}

All submissions must follow the specified format.

\subsection{Dimensions}




The text of the paper should be formatted in two columns, with an
overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
between the columns. The left margin should be 0.75~inches and the top
margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
whether you print on US letter or A4 paper, but all final versions
must be produced for US letter size.
Do not write anything on the margins.

The paper body should be set in 10~point type with a vertical spacing
of 11~points. Please use Times typeface throughout the text.

\subsection{Title}

The paper title should be set in 14~point bold type and centered
between two horizontal rules that are 1~point thick, with 1.0~inch
between the top rule and the top edge of the page. Capitalize the
first letter of content words and put the rest of the title in lower
case.

\subsection{Author Information for Submission}
\label{author info}

ICML uses double-blind review, so author information must not appear. If
you are using \LaTeX\/ and the \texttt{icml2024.sty} file, use
\verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
will not be printed unless \texttt{accepted} is passed as an argument to the
style file.
Submissions that include the author information will not
be reviewed.

\subsubsection{Self-Citations}

If you are citing published papers for which you are an author, refer
to yourself in the third person. In particular, do not use phrases
that reveal your identity (e.g., ``in previous work \cite{langley00}, we
have shown \ldots'').

Do not anonymize citations in the reference section. The only exception are manuscripts that are
not yet published (e.g., under submission). If you choose to refer to
such unpublished manuscripts \cite{anonymous}, anonymized copies have
to be submitted
as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
paper should be self contained and should contain sufficient detail
for the reviewers to evaluate the work. In particular, reviewers are
not required to look at the Supplementary Material when writing their
review (they are not required to look at more than the first $8$ pages of the submitted document).

\subsubsection{Camera-Ready Author Information}
\label{final author}

If a paper is accepted, a final camera-ready copy must be prepared.
%
For camera-ready papers, author information should start 0.3~inches below the
bottom rule surrounding the title. The authors' names should appear in 10~point
bold type, in a row, separated by white space, and centered. Author names should
not be broken across lines. Unbolded superscripted numbers, starting 1, should
be used to refer to affiliations.

Affiliations should be numbered in the order of appearance. A single footnote
block of text should be used to list all the affiliations. (Academic
affiliations should list Department, University, City, State/Region, Country.
Similarly for industrial affiliations.)

Each distinct affiliations should be listed once. If an author has multiple
affiliations, multiple superscripts should be placed after the name, separated
by thin spaces. If the authors would like to highlight equal contribution by
multiple first authors, those authors should have an asterisk placed after their
name in superscript, and the term ``\textsuperscript{*}Equal contribution"
should be placed in the footnote block ahead of the list of affiliations. A
list of corresponding authors and their emails (in the format Full Name
\textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
Ideally only one or two names should be listed.

A sample file with author names is included in the ICML2024 style file
package. Turn on the \texttt{[accepted]} option to the stylefile to
see the names rendered. All of the guidelines above are implemented
by the \LaTeX\ style file.

\subsection{Abstract}

The paper abstract should begin in the left column, 0.4~inches below the final
address. The heading `Abstract' should be centered, bold, and in 11~point type.
The abstract body should use 10~point type, with a vertical spacing of
11~points, and should be indented 0.25~inches more than normal on left-hand and
right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
sentences. Gross violations will require correction at the camera-ready phase.

\subsection{Partitioning the Text}

You should organize your paper into sections and paragraphs to help
readers place a structure on the material and understand its
contributions.

\subsubsection{Sections and Subsections}

Section headings should be numbered, flush left, and set in 11~pt bold
type with the content words capitalized. Leave 0.25~inches of space
before the heading and 0.15~inches after the heading.

Similarly, subsection headings should be numbered, flush left, and set
in 10~pt bold type with the content words capitalized. Leave
0.2~inches of space before the heading and 0.13~inches afterward.

Finally, subsubsection headings should be numbered, flush left, and
set in 10~pt small caps with the content words capitalized. Leave
0.18~inches of space before the heading and 0.1~inches after the
heading.

Please use no more than three levels of headings.

\subsubsection{Paragraphs and Footnotes}

Within each section or subsection, you should further partition the
paper into paragraphs. Do not indent the first line of a given
paragraph, but insert a blank line between succeeding ones.

You can use footnotes\footnote{Footnotes
should be complete sentences.} to provide readers with additional
information about a topic without interrupting the flow of the paper.
Indicate footnotes with a number in the text where the point is most
relevant. Place the footnote in 9~point type at the bottom of the
column in which it appears. Precede the first footnote in a column
with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
appear in each column, in the same order as they appear in the text,
but spread them across columns and pages if possible.}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
\caption{Historical locations and number of accepted papers for International
Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
produced, the number of accepted papers for ICML 2008 was unknown and instead
estimated.}
\label{icml-historical}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{Figures}

You may want to include figures in the paper to illustrate
your approach and results. Such artwork should be centered,
legible, and separated from the text. Lines should be dark and at
least 0.5~points thick for purposes of reproduction, and text should
not appear on a gray background.

Label all distinct components of each figure. If the figure takes the
form of a graph, then give a name for each axis and include a legend
that briefly describes each curve. Do not include a title inside the
figure; instead, the caption should serve this function.

Number figures sequentially, placing the figure number and caption
\emph{after} the graphics, with at least 0.1~inches of space before
the caption and 0.1~inches after it, as in
\cref{icml-historical}. The figure caption should be set in
9~point type and centered unless it runs two or more lines, in which
case it should be flush left. You may float figures to the top or
bottom of a column, and you may set wide figures across both columns
(use the environment \texttt{figure*} in \LaTeX). Always place
two-column figures at the top or bottom of the page.

\subsection{Algorithms}

If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
environments to format pseudocode. These require
the corresponding stylefiles, algorithm.sty and
algorithmic.sty, which are supplied with this package.
\cref{alg:example} shows an example.

\begin{algorithm}[tb]
   \caption{Bubble Sort}
   \label{alg:example}
\begin{algorithmic}
   \STATE {\bfseries Input:} data $x_i$, size $m$
   \REPEAT
   \STATE Initialize $noChange = true$.
   \FOR{$i=1$ {\bfseries to} $m-1$}
   \IF{$x_i > x_{i+1}$}
   \STATE Swap $x_i$ and $x_{i+1}$
   \STATE $noChange = false$
   \ENDIF
   \ENDFOR
   \UNTIL{$noChange$ is $true$}
\end{algorithmic}
\end{algorithm}

\subsection{Tables}

You may also want to include tables that summarize material. Like
figures, these should be centered, legible, and numbered consecutively.
However, place the title \emph{above} the table with at least
0.1~inches of space before the title and the same after it, as in
\cref{sample-table}. The table title should be set in 9~point
type and centered unless it runs two or more lines, in which case it
should be flush left.

% Note use of \abovespace and \belowspace to get reasonable spacing
% above and below tabular lines.

\begin{table}[t]
\caption{Classification accuracies for naive Bayes and flexible
Bayes on various data sets.}
\label{sample-table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
Data set & Naive & Flexible & Better? \\
\midrule
Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Tables contain textual material, whereas figures contain graphical material.
Specify the contents of each row and column in the table's topmost
row. Again, you may float tables to a column's top or bottom, and set
wide tables across both columns. Place two-column tables at the
top or bottom of the page.

\subsection{Theorems and such}
The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
\begin{definition}
\label{def:inj}
A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
\end{definition}
Using \cref{def:inj} we immediate get the following result:
\begin{proposition}
If $f$ is injective mapping a set $X$ to another set $Y$, 
the cardinality of $Y$ is at least as large as that of $X$
\end{proposition}
\begin{proof} 
Left as an exercise to the reader. 
\end{proof}
\cref{lem:usefullemma} stated next will prove to be useful.
\begin{lemma}
\label{lem:usefullemma}
For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
\end{lemma}
\begin{theorem}
\label{thm:bigtheorem}
If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
\end{theorem}
An easy corollary of \cref{thm:bigtheorem} is the following:
\begin{corollary}
If $f:X\to Y$ is bijective, 
the cardinality of $X$ is at least as large as that of $Y$.
\end{corollary}
\begin{assumption}
The set $X$ is finite.
\label{ass:xfinite}
\end{assumption}
\begin{remark}
According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
\end{remark}
%restatable

\subsection{Citations and References}

Please use APA reference format regardless of your formatter
or word processor. If you rely on the \LaTeX\/ bibliographic
facility, use \texttt{natbib.sty} and \texttt{icml2024.bst}
included in the style-file package to obtain this format.

Citations within the text should include the authors' last names and
year. If the authors' names are included in the sentence, place only
the year in parentheses, for example when referencing Arthur Samuel's
pioneering work \yrcite{Samuel59}. Otherwise place the entire
reference in parentheses with the authors and year separated by a
comma \cite{Samuel59}. List multiple references separated by
semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
construct only for citations with three or more authors or after
listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

Authors should cite their own work in the third person
in the initial version of their paper submitted for blind review.
Please refer to \cref{author info} for detailed instructions on how to
cite your own papers.

Use an unnumbered first-level section heading for the references, and use a
hanging indent style, with the first line of the reference flush against the
left margin and subsequent lines indented by 10 points. The references at the
end of this document give examples for journal articles \cite{Samuel59},
conference publications \cite{langley00}, book chapters \cite{Newell81}, books
\cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
\cite{mitchell80}, and dissertations \cite{kearns89}.

Alphabetize references by the surnames of the first authors, with
single author entries preceding multiple author entries. Order
references for the same authors by year of publication, with the
earliest first. Make sure that each reference includes all relevant
information (e.g., page numbers).

Please put some effort into making references complete, presentable, and
consistent, e.g. use the actual current name of authors.
If using bibtex, please protect capital letters of names and
abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
in your .bib file.

\section*{Accessibility}
Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

\section*{Software and Data}

If a paper is accepted, we strongly encourage the publication of software and data with the
camera-ready version of the paper whenever appropriate. This can be
done by including a URL in the camera-ready copy. However, \textbf{do not}
include URLs that reveal your institution or identity in your
submission for review. Instead, provide an anonymous URL or upload
the material as ``Supplementary Material'' into the OpenReview reviewing
system. Note that reviewers are not required to look at this material
when writing their review.

% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}

\textbf{Do not} include acknowledgements in the initial version of
the paper submitted for blind review.

If a paper is accepted, the final camera-ready version can (and
probably should) include acknowledgements. In this case, please
place such acknowledgements in an unnumbered section at the
end of the paper. Typically, this will include thanks to reviewers
who gave useful comments, to colleagues who contributed to the ideas,
and to funding agencies and corporate sponsors that provided financial
support.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{You \emph{can} have an appendix here.}

You can have as much text here as you want. The main body must be at most $8$ pages long.
For the final version, one more page can be added.
If you want, you can use an appendix like this one.  

The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
